{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\mcm23\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/15/aa/3f4c7bcee2057a76562a5b33ecbd199be08cdb4443a02e26bd2c3cf6fc39/pip-23.3.2-py3-none-any.whl.metadata\n",
      "  Downloading pip-23.3.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 2.1/2.1 MB 8.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-23.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from util import load_data, separate_data\n",
    "from models.graphcnn import GraphCNN\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(args, model, device, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = args.iters_per_epoch\n",
    "    pbar = tqdm(range(total_iters), unit='batch')\n",
    "\n",
    "    loss_accum = 0\n",
    "    for pos in pbar:\n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:args.batch_size]\n",
    "\n",
    "        batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "        output = model(batch_graph)\n",
    "\n",
    "        labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        loss_accum += loss\n",
    "\n",
    "        #report\n",
    "        pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/total_iters\n",
    "    print(\"loss training: %f\" % (average_loss))\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
    "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    idx = np.arange(len(graphs))\n",
    "    for i in range(0, len(graphs), minibatch_size):\n",
    "        sampled_idx = idx[i:i+minibatch_size]\n",
    "        if len(sampled_idx) == 0:\n",
    "            continue\n",
    "        output.append(model([graphs[j] for j in sampled_idx]).detach())\n",
    "    return torch.cat(output, 0)\n",
    "\n",
    "def test(args, model, device, train_graphs, test_graphs, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    output = pass_data_iteratively(model, train_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in train_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_train = correct / float(len(train_graphs))\n",
    "\n",
    "    output = pass_data_iteratively(model, test_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in test_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_test = correct / float(len(test_graphs))\n",
    "\n",
    "    print(\"accuracy train: %f test: %f\" % (acc_train, acc_test))\n",
    "\n",
    "    return acc_train, acc_test\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    # Note: Hyper-parameters need to be tuned in order to obtain results reported in the paper.\n",
    "    parser = argparse.ArgumentParser(description='PyTorch graph convolutional neural net for whole-graph classification')\n",
    "    parser.add_argument('--dataset', type=str, default=\"MUTAG\",\n",
    "                        help='name of dataset (default: MUTAG)')\n",
    "    parser.add_argument('--device', type=int, default=0,\n",
    "                        help='which gpu to use if any (default: 0)')\n",
    "    parser.add_argument('--batch_size', type=int, default=32,\n",
    "                        help='input batch size for training (default: 32)')\n",
    "    parser.add_argument('--iters_per_epoch', type=int, default=50,\n",
    "                        help='number of iterations per each epoch (default: 50)')\n",
    "    parser.add_argument('--epochs', type=int, default=350,\n",
    "                        help='number of epochs to train (default: 350)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--seed', type=int, default=0,\n",
    "                        help='random seed for splitting the dataset into 10 (default: 0)')\n",
    "    parser.add_argument('--fold_idx', type=int, default=0,\n",
    "                        help='the index of fold in 10-fold validation. Should be less then 10.')\n",
    "    parser.add_argument('--num_layers', type=int, default=5,\n",
    "                        help='number of layers INCLUDING the input one (default: 5)')\n",
    "    parser.add_argument('--num_mlp_layers', type=int, default=2,\n",
    "                        help='number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=64,\n",
    "                        help='number of hidden units (default: 64)')\n",
    "    parser.add_argument('--final_dropout', type=float, default=0.5,\n",
    "                        help='final layer dropout (default: 0.5)')\n",
    "    parser.add_argument('--graph_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\"],\n",
    "                        help='Pooling for over nodes in a graph: sum or average')\n",
    "    parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\", \"max\"],\n",
    "                        help='Pooling for over neighboring nodes: sum, average or max')\n",
    "    parser.add_argument('--learn_eps', action=\"store_true\",\n",
    "                                        help='Whether to learn the epsilon weighting for the center nodes. Does not affect training accuracy though.')\n",
    "    parser.add_argument('--degree_as_tag', action=\"store_true\",\n",
    "    \t\t\t\t\thelp='let the input node features be the degree of nodes (heuristics for unlabeled graph)')\n",
    "    parser.add_argument('--filename', type = str, default = \"\",\n",
    "                                        help='output file')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #set up seeds and gpu device\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)    \n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    graphs, num_classes = load_data(args.dataset, args.degree_as_tag)\n",
    "\n",
    "    ##10-fold cross validation. Conduct an experiment on the fold specified by args.fold_idx.\n",
    "    train_graphs, test_graphs = separate_data(graphs, args.seed, args.fold_idx)\n",
    "\n",
    "    model = GraphCNN(args.num_layers, args.num_mlp_layers, train_graphs[0].node_features.shape[1], args.hidden_dim, num_classes, args.final_dropout, args.learn_eps, args.graph_pooling_type, args.neighbor_pooling_type, device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss = train(args, model, device, train_graphs, optimizer, epoch)\n",
    "        acc_train, acc_test = test(args, model, device, train_graphs, test_graphs, epoch)\n",
    "\n",
    "        if not args.filename == \"\":\n",
    "            with open(args.filename, 'w') as f:\n",
    "                f.write(\"%f %f %f\" % (avg_loss, acc_train, acc_test))\n",
    "                f.write(\"\\n\")\n",
    "        print(\"\")\n",
    "\n",
    "        print(model.eps)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
